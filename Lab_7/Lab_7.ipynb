{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "**Creator**: Lauren Enriquez<br>\n",
    "**Date**: October 23, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the data\n",
    "Here, 100 MB sets of random data containng varying amounts of zeros were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p100 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[1, 0])\n",
    "p100 = np.packbits(p100)\n",
    "\n",
    "p90 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[0.9, 0.1])\n",
    "p90 = np.packbits(p90)\n",
    "\n",
    "p80 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[0.8, 0.2])\n",
    "p80 = np.packbits(p80)\n",
    "\n",
    "p70 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[0.7, 0.3])\n",
    "p70 = np.packbits(p70)\n",
    "\n",
    "p60 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[0.6, 0.4])\n",
    "p60 = np.packbits(p60)\n",
    "\n",
    "p50 = np.random.choice([0, 1], size=8*1000000, replace=True, p=[0.5, 0.5])\n",
    "p50 = np.packbits(p50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"p100\", \"wb\").write(p100)\n",
    "open(\"p90\", \"wb\").write(p90)\n",
    "open(\"p80\", \"wb\").write(p80)\n",
    "open(\"p70\", \"wb\").write(p70)\n",
    "open(\"p60\", \"wb\").write(p60)\n",
    "open(\"p50\", \"wb\").write(p50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DNA and Protein Sequence\n",
    "DNA and protein sequences at 100 million letters long were created.\n",
    "Each letter has the same probability of being selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = np.random.choice([\"A\", \"T\", \"G\", \"C\"],\n",
    "                       size=100000000,\n",
    "                       replace=True,\n",
    "                       p=[0.25, 0.25, 0.25, 0.25]);\n",
    "open(\"dna.fa\", 'w').write(\"\".join(dna));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = np.random.choice([\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"W\",\"Y\",\"V\"],\n",
    "                           size=100000000,\n",
    "                           replace=True,\n",
    "                           p=np.repeat(0.05, 20));\n",
    "open(\"protein.fa\", 'w').write(\"\".join(protein));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing the data\n",
    "\n",
    "Commands similar to the following were run on terminal:\n",
    "```\n",
    "time gzip –c zeros_100p > zeros_100p.gz\n",
    "time bzip2 –k zeros_100p\n",
    "time pbzip2 –c zeros_100p > zeros_100p_pbzip.bz2\n",
    "```\n",
    "This was done for each random file generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original File = p100 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Original File = p90\n",
    "\n",
    "|command type|input file size| output file size| time elapsed||| |||||||||| |||command type|input file size| output file size| time elapsed|\n",
    "|----------|----------|----------|----------|||||||||||| ||| |----------|----------|----------|----------|\n",
    "|gzip|1 MB|1.01 kB|0.007 s| |||||||||||| |||gzip|1 MB|561 kB|0.263 s|\n",
    "|bzip2|1 MB|48 B|0.011 s| ||||||||| ||||||bzip2|1 MB|584 kB|0.102 s|\n",
    "|bpzip2|1 MB|97 B|0.014 s| ||||||||||| |||| bpzip2|1 MB|584 kB|0.097 s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original File = p80 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Original File = p70\n",
    "\n",
    "|command type|input file size| output file size| time elapsed||| |||||||||| |||command type|input file size| output file size| time elapsed|\n",
    "|----------|----------|----------|----------|||||||||||| ||| |----------|----------|----------|----------|\n",
    "|gzip|1 MB|774 kB|0.170 s| |||||||||||| |||gzip|1 MB|893 kB|0.138 s|\n",
    "|bzip2|1 MB|827 kB|0.108 s| ||||||||| ||||||bzip2|1 MB|952 kB|0.116 s|\n",
    "|pbzip2|1 MB|827 kB|0.106 s| ||||||||||| |||| pbzip2|1 MB|952 kB|0.123 s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original File = p60 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Original File = p50\n",
    "\n",
    "|command type|input file size| output file size| time elapsed||| |||||||||| |||command type|input file size| output file size| time elapsed|\n",
    "|----------|----------|----------|----------|||||||||||| ||| |----------|----------|----------|----------|\n",
    "|gzip|1 MB|977 kB|0.044 s| |||||||||||| |||gzip|1 MB|1 MB|0.036 s|\n",
    "|bzip2|1 MB|1 MB|0.128 s| ||||||||| ||||||bzip2|1 MB|1 MB|0.134 s|\n",
    "|pbzip2|1 MB|1 MB|0.134 s| ||||||||||| |||| pbzip2|1 MB|1 MB|0.142 s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original File = dna.fa &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Original File = protein.fa\n",
    "\n",
    "|command type|input file size| output file size| time elapsed||| |||||||||| |||command type|input file size| output file size| time elapsed|\n",
    "|----------|----------|----------|----------|||||||||||| ||| |----------|----------|----------|----------|\n",
    "|gzip|100 MB|29.2 MB|23.577 s| |||||||||||| |||gzip|100 MB|60.6 MB|4.346 s|\n",
    "|bzip2|100 MB|27.3 MB|9.522 s| ||||||||| ||||||bzip2|100 MB|55.3 MB|9.580 s|\n",
    "|pbzip2|100 MB|27.3 MB|1.013 s| ||||||||||| |||| pbzip2|100 MB|55.3 MB|1.037 s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "**Which algorithm achieves the best level of compression on each file type?**<br> \n",
    "\n",
    "|File type| Best compression|\n",
    "|------|-------|\n",
    "|p100| pbzip2|\n",
    "|p90|pbzip2 |\n",
    "|p80|pbzip2 |\n",
    "|p70|bzip2 |\n",
    "|p60|gzip |\n",
    "|p50|gzip |\n",
    "|dna.fa|pbzip2|\n",
    "|protein.fa|pbzip2|\n",
    "\n",
    "\n",
    "**Which algorithm is the fastest?**<br> pzip2 runs the fastest on average according to the data generated above.\n",
    "\n",
    "**What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?** <br> pbzip2 runs significantly faster than bzip2. bzip2 compresses files of mostly zeroes better than pbzip2. pbzip is faster because it runs in parallel.\n",
    "\n",
    "**How does the level of compression change as the percentage of zeros increases? Why does this happen?**<br> The compression of the file becomes less effective as the percentage of zeroes increases. This is because there are fewer sections of uninterrupted entries of the same kind that could be compressed.\n",
    "\n",
    "**What is the minimum number of bits required to store a single DNA base?**<br> 2 bits.\n",
    "\n",
    "**What is the minimum number of bits required to store an amino acid letter?** <br> 4.322 (rounded to 5) according to lecture slides.\n",
    "\n",
    "**In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?**<br> gzip and bzip2 took 29.2 and 27.3 MB respectively for the DNA sequence. gzip and bzip2 took 60.6 and 55.3 MB respectively for the protein sequence.\n",
    "\n",
    "**Are gzip and bzip2 performing well on DNA and protein?** <br> They are doing really quite well, they are just a few MB off from the ideal 25MB for DNA. The protein sequence was at 60.6 MB for gzip and 55.3 MB for bzip2 instead of the ideal 50 MB to encode 100MB of protein data. gzip and bzip2 perform better on the DNA than the protein.\n",
    "<br><br>\n",
    "\n",
    "## Compressing real data\n",
    "Now we find 20 sequences of gp120 homologs from HIV isolates and concatenate them into one fasta file gp120.fasta.\n",
    "\n",
    "**A priori, do you expect to achieve better or worse compression here than random data? Why?** <br> It is expected that we can achieve better compression than random data since homologous sequences will have large sections of similar data (more patterns in the data can be found)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = 'lauren_enriquez@berkeley.edu'\n",
    "handle = Entrez.esearch(db='nucleotide',\n",
    "                       term = 'gp120 HIV',\n",
    "                       sort = 'relevance',\n",
    "                       idtype = 'acc',\n",
    "                       retmax = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gp120.fasta', 'w') as fasta_handle:\n",
    "    for i in Entrez.read(handle)['IdList']:\n",
    "        handle = Entrez.efetch(db='nucleotide',\n",
    "                               id=i,\n",
    "                               rettype = 'fasta',\n",
    "                               retmode='text')\n",
    "        new_handle = SeqIO.parse(handle, 'fasta')\n",
    "        SeqIO.write(new_handle, fasta_handle, 'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gp120 homologs from HIV isolates\n",
    "|command type|input file size| output file size| time elapsed|\n",
    "|----------|----------|----------|----------|\n",
    "|gzip|12.1 kB|1.45 kB|0.001 s|\n",
    "|bzip2|12.1 kB|1.58 kB|0.003 s|\n",
    "|bpzip2|12.1 kB|1.58 kB|0.004 s|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the compression ratio of this file compare to random data?** <br>\n",
    "We see here that we get compression ratios **0.120** for gzip and **0.131** bzip2 and pbzip2.<br> For gzip and bzip2, this ratio is about **half** that than for the **random DNA data**, which had compression ratios of **0.292** for gzip and **0.273** for bzip.\n",
    "\n",
    "\n",
    "## Estimating Compression of 1000 terabytes\n",
    "\n",
    "**Background**<br>\n",
    "You’re working at a biotech company that generates 1000 terabytes of data every day. In a meeting, your boss mentions that it costs the company $50\n",
    "\n",
    "per terabyte of hard disk space, and so every 1% reduction in data that must be stored translates into a $500 savings per day. Your team will get a bonus this year equal to the amount of savings you’re able to generate by compressing the company’s data.<br><br>\n",
    "Incentivized by this bonus, your team gets to work determining which compression algorithm will generate the most savings. The algorithm you choose must either be quick enough to compress 1000 terabytes a day, or efficient enough that even if all the data isn’t compressed, the savings are maximized by the data that you have time to compress.<br><br>\n",
    "Let’s make some assumptions about the contents of the data at your biotech company. Most of the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each other. Another 10% might be protein sequences, and the last 10% are binary microscope images which we’ll assume follow the worst-case scenario of being completely random.<br><br>\n",
    "**Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?** <br>\n",
    "\n",
    "For **re-sequencing of similar genomes**, I would use **pbzip2** for parallel compression.\n",
    "\n",
    "For **protein sequences**,I would also use **pbzip2** would also work fastest among the algorithms, and provide approximately the same compression as gzip2.\n",
    "\n",
    "For the last 10% of **completely random binary microscope images**, **gzip2** provides marginally the best compression (usually for most random sequences).\n",
    "\n",
    "Thus, we can see approximately 85% data storage savings on 80% of the genome data, 70% data storage savings when compressing the 10% protein sequences, and at worst 3% savings when compressing the 10% random images. Thus, it is estimated that a savings of around **78% data storage and thus a $39,000 every day** will be done using this compression scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
